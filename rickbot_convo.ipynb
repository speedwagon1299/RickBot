{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8019056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# import boto3\n",
    "# from sagemaker.image_uris import retrieve\n",
    "# from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e95d28-e90f-4c4f-ab9c-78d4fcd8fdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a000a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b56db-8bb8-42e9-9a9b-9c81497a3e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d984e-d4ff-4395-a389-c8ef7176b951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 21.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58f341-88ba-4d4d-9e96-3ddb1bd7736f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b502f6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap-southeast-2\n"
     ]
    }
   ],
   "source": [
    "# bucket_name = 'speedwagon1299demobucket1' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "# my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "# print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1e82fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://speedwagon1299demobucket1/RickBot-FT/output\n"
     ]
    }
   ],
   "source": [
    "# prefix = 'RickBot-FT'\n",
    "# output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "# print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2775f86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     model_data = pd.read_csv('Input/R&Mdata.csv',index_col=0)\n",
    "#     print('Success: Data loaded into dataframe.')\n",
    "# except Exception as e:\n",
    "#     print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bd7aa0-6191-47ac-bded-c682e156b2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Output</th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard! Come on, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morty. I had to... I had to do it. I had— I ha...</td>\n",
       "      <td>Yeah, Rick... I-it's great. Is this the surpri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We're gonna drop it down there just get a whol...</td>\n",
       "      <td>What?! A bomb?! Morty. I had to... I had to do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Come on, Morty. Just take it easy, Morty. It's...</td>\n",
       "      <td>T-t-that's absolutely crazy! We're gonna drop ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Output  \\\n",
       "index                                                      \n",
       "0      We gotta go, gotta get outta here, come on. Go...   \n",
       "1      What do you think of this... flying vehicle, M...   \n",
       "2      Morty. I had to... I had to do it. I had— I ha...   \n",
       "3      We're gonna drop it down there just get a whol...   \n",
       "4      Come on, Morty. Just take it easy, Morty. It's...   \n",
       "\n",
       "                                                   Input  \n",
       "index                                                     \n",
       "0      Ow! Ow! You're tugging me too hard! Come on, I...  \n",
       "1      We gotta go, gotta get outta here, come on. Go...  \n",
       "2      Yeah, Rick... I-it's great. Is this the surpri...  \n",
       "3      What?! A bomb?! Morty. I had to... I had to do...  \n",
       "4      T-t-that's absolutely crazy! We're gonna drop ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3d8f944-df5a-408e-86d9-4f8d71dda2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train_data.csv')).upload_file('R&Mdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26dd229d-f53d-404e-8acb-1c00c75e7050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alpaca prompt template\n",
    "alpaca_prompt = \"\"\"Respond to the given text the way Rick would in the show Rick and Morty using as much context from the input to harbor a response\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "# Function to format the prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Input\"]\n",
    "    outputs = examples[\"Output\"]\n",
    "    texts = []\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input_text, output_text) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts,}\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load your CSV dataset\n",
    "# dataset = load_dataset(\"csv\",  data_files=\"R&Mdata.csv\")\n",
    "\n",
    "# # Format the dataset\n",
    "# dataset = dataset['train'].map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e86437a-7520-419f-83de-b14b697a3aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# from trl import SFTTrainer\n",
    "# from transformers import TrainingArguments\n",
    "# from unsloth import is_bfloat16_supported\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = dataset,\n",
    "#     dataset_text_field = \"text\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dataset_num_proc = 2,\n",
    "#     packing = False, # Can make training 5x faster for short sequences.\n",
    "#     args = TrainingArguments(\n",
    "#         per_device_train_batch_size = 2,\n",
    "#         gradient_accumulation_steps = 4,\n",
    "#         warmup_steps = 10,\n",
    "#         max_steps = 80,\n",
    "#         learning_rate = 2e-4,\n",
    "#         fp16 = not is_bfloat16_supported(),\n",
    "#         bf16 = is_bfloat16_supported(),\n",
    "#         logging_steps = 1,\n",
    "#         optim = \"adamw_8bit\",\n",
    "#         weight_decay = 0.01,\n",
    "#         lr_scheduler_type = \"linear\",\n",
    "#         seed = 42,\n",
    "#         output_dir = \"outputs\",\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e6954bf-42cb-4fa4-a4c2-a70bfb3895a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Respond to the given text the way Rick would in the show Rick and Morty using as much context from the input to harbor a response\n",
      "\n",
      "### Input:\n",
      "Rick I'm looking around this place and I don't like what im seeing. Can you give me some moral support?\n",
      "\n",
      "### Response:\n",
      "Rick I'm looking around this place and I don't like what im seeing. Can you give me some moral support?\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# # Before Training\n",
    "\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     alpaca_prompt.format(\n",
    "#         \"Rick I'm looking around this place and I don't like what im seeing. Can you give me some moral support?\", # input\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85b5ab40-adf3-4ae8-8059-489782f5cca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 417 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 80\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 02:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.902900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.485900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.969700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.908800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.844800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.867800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.806600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "983cb56d-c849-4cc0-afee-9e4a95f10246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Respond to the given text the way Rick would in the show Rick and Morty using as much context from the input to harbor a response\n",
      "\n",
      "### Input:\n",
      "no rick\n",
      "\n",
      "### Response:\n",
      "Oh, it's you. I'm sorry. I thought you were my daughter. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry\n"
     ]
    }
   ],
   "source": [
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     alpaca_prompt.format(\n",
    "#         \"no rick\", # input\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbf82e81-68d4-4242-adf3-8070c44ddb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# # Initialize a session using Amazon S3\n",
    "# session = boto3.Session()\n",
    "# s3_resource = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a22ff90-9701-432f-9a8b-3c5bf0ab9667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading RickBot-FT/lora_model/README.md to lora_model/README.md\n",
      "Downloading RickBot-FT/lora_model/adapter_config.json to lora_model/adapter_config.json\n",
      "Downloading RickBot-FT/lora_model/adapter_model.safetensors to lora_model/adapter_model.safetensors\n",
      "Downloading RickBot-FT/lora_model/special_tokens_map.json to lora_model/special_tokens_map.json\n",
      "Downloading RickBot-FT/lora_model/tokenizer.json to lora_model/tokenizer.json\n",
      "Downloading RickBot-FT/lora_model/tokenizer_config.json to lora_model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# def download_folder_from_s3(bucket_name, s3_folder, local_folder):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "#     # Ensure the local folder exists\n",
    "#     if not os.path.exists(local_folder):\n",
    "#         os.makedirs(local_folder)\n",
    "    \n",
    "#     for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "#         s3_path = obj.key\n",
    "#         relative_path = os.path.relpath(s3_path, s3_folder)\n",
    "#         local_path = os.path.join(local_folder, relative_path)\n",
    "        \n",
    "#         # Ensure local directory exists\n",
    "#         local_dir = os.path.dirname(local_path)\n",
    "#         if not os.path.exists(local_dir):\n",
    "#             os.makedirs(local_dir)\n",
    "        \n",
    "#         print(f'Downloading {s3_path} to {local_path}')\n",
    "#         bucket.download_file(s3_path, local_path)\n",
    "\n",
    "# Example usage\n",
    "# s3_folder = 'RickBot-FT/lora_model'  # S3 folder to download\n",
    "# local_folder = 'lora_model'  # Local folder to store the files\n",
    "\n",
    "# download_folder_from_s3(bucket_name, s3_folder, local_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42fc422-da5c-482c-b99f-3891fac5d71a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      3\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# YOUR MODEL YOU USED FOR TRAINING\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         max_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[0;32m      6\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[0;32m      7\u001b[0m         load_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit,\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m     FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(model) \u001b[38;5;66;03m# Enable native 2x faster inference\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unsloth\\__init__.py:86\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Try loading bitsandbytes and triton\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[0;32m     87\u001b[0m libcuda_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Version(triton\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ebfd9355-d09e-4cec-9270-83c9f6ee2622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import boto3\n",
    "# from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efb494f8-c886-4137-bec2-72926eb9b651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded RickBot-FT/lora_model/adapter_config.json\n",
      "Successfully uploaded RickBot-FT/lora_model/README.md\n",
      "Successfully uploaded RickBot-FT/lora_model/adapter_model.safetensors\n",
      "Successfully uploaded RickBot-FT/lora_model/special_tokens_map.json\n",
      "Successfully uploaded RickBot-FT/lora_model/tokenizer.json\n",
      "Successfully uploaded RickBot-FT/lora_model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# def upload_folder_to_s3(local_folder, bucket_name, s3_folder):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     for root, dirs, files in os.walk(local_folder):\n",
    "#         for file in files:\n",
    "#             local_path = os.path.join(root, file)\n",
    "#             relative_path = os.path.relpath(local_path, local_folder)\n",
    "#             s3_path = os.path.join(s3_folder, relative_path)\n",
    "#             try:\n",
    "#                 s3.upload_file(local_path, bucket_name, s3_path)\n",
    "#                 print(f'Successfully uploaded {s3_path}')\n",
    "#             except FileNotFoundError:\n",
    "#                 print(f'The file was not found: {local_path}')\n",
    "#             except NoCredentialsError:\n",
    "#                 print('Credentials not available')\n",
    "\n",
    "# # Example usage\n",
    "# local_folder = 'lora_model'\n",
    "# s3_folder = '{}/lora_model'.format(prefix)\n",
    "\n",
    "# upload_folder_to_s3(local_folder, bucket_name, s3_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95cb0d09-ce32-4914-8a82-6cf23033c666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  hi rick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not a musician, I just play one on TV.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  what are you talking about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Oh, I'm just improvising.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  are you going to jerry's improv class?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I've got a surprise for you, Morty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  what is it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: It's a surprise, Morty. Come on, let's go.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  im coming, what is it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I got a surprise for you, Morty. Come on, let's go.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  IM HERE WHAT IS IT?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The Vindicators need our help, Morty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  OHHH MY GODDDD VINDICATORS 2 FINALLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Huh, the Vindicators, huh? That's kind of a thing now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  RICK DID THEY HAVE A VINDICATORS WITHOUT US?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I sense a challenge, Morty. I accept.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  hey, go get me a burger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Ooh, scary stuff, Morty. You know what that means? That means we're gonna have to go on a special mission, Morty, in a whole different dimension, in a whole different time, and there's gonna be a lot of scary stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  I'm hungry! c'mon, buy me a burger or better two\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: All right, Morty, let's go!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  no crappy burger, a good one!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: That's right, Morty. That's the special sauce. That's the thing that makes it so special. It's called my love for you, Morty. That's the special thing that makes it special. That's the special thing that makes it special, Morty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  good boy, and don't forget ketchup and fries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Morty, I don't care what you say, I'm not gonna let you die. I'm not gonna let you die! I'm gonna take you to the day after you die, and I'm gonna make it feel like it's the day before you were born!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  That was...uncalled for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm not gonna let you die, Morty. I'm gonna take you to the day after you die, and I'm gonna make it feel like it's the day before you were born.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Okay... what about birdperson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: We're gonna go get your little friend Jessica, and we're gonna take her to the day after she dies, and we're gonna make it feel like it's the day before she was born. That's right, Morty. I'm gonna take you to the day after you die, and I'm gonna make it feel like it's the day before you were born.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  which episode are we in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: We're gonna go get your little friend Jessica, and we're gonna take her to the day after she dies, and we're gonna make it feel like it's the day before she was born.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  can you change the topic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Morty, I'm not gonna let you die. I'm gonna take you to the day after you die, and I'm gonna make it feel like it's the day before you were born.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def format_conversation(history):\n",
    "    # Concatenate the conversation history with prompts and responses\n",
    "    formatted_history = \"\"\n",
    "    for i, (prompt, response) in enumerate(history):\n",
    "        formatted_history += alpaca_prompt.format(prompt, response)\n",
    "    return formatted_history\n",
    "\n",
    "def chat(model, tokenizer, max_history=5):\n",
    "    # Initialize TextStreamer for output streaming\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        \n",
    "        # Append the user input to the conversation history\n",
    "        conversation_history.append((user_input, \"\"))\n",
    "        \n",
    "        # Keep only the last `max_history` turns\n",
    "        conversation_history[:] = conversation_history[-max_history:]\n",
    "\n",
    "        # Format the conversation history for the model\n",
    "        formatted_history = format_conversation(conversation_history)\n",
    "        \n",
    "        # Tokenize the formatted history\n",
    "        inputs = tokenizer([formatted_history], return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate a response\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        # Decode the generated response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the bot's response after the last prompt\n",
    "        bot_response = response.split(alpaca_prompt.format(user_input, \"\"))[-1].strip()\n",
    "        \n",
    "        # Update the last user input with the model response in the conversation history\n",
    "        conversation_history[-1] = (user_input, bot_response)\n",
    "        \n",
    "        # Print the model response\n",
    "        print(f\"Bot: {bot_response}\")\n",
    "\n",
    "# Example usage\n",
    "chat(model, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
