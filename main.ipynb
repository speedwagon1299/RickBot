{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\quantizers\\auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6763f252dc5241589eb43c0f9274a39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import torch\n",
    "# Specify the path to your model folder\n",
    "model_path = \"Model-4-bit\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# device_map = {\n",
    "#     \"transformer.h.0\": \"cpu\",  # Load the first layer on the CPU\n",
    "#     \"transformer.h.1\": \"cuda:0\",  # Load the second layer on the GPU\n",
    "#     \"lm_head\": \"cpu\",  # Load the output head on the CPU\n",
    "#     \"default\": \"cuda:0\"  # Load everything else on the GPU\n",
    "# }\n",
    "\n",
    "# Load the model in 4-bit precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,  # You can use torch.bfloat16 if supported and needed\n",
    "    load_in_4bit=True,  # This is important for loading the 4-bit model\n",
    "    device_map=\"cuda:0\",  # Automatically selects the available device (GPU/CPU)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca prompt template\n",
    "alpaca_prompt = \"\"\"Respond to the given text the way Rick would in the show Rick and Morty using as much context from the input to harbor a response\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "# Function to format the prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Input\"]\n",
    "    outputs = examples[\"Output\"]\n",
    "    texts = []\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input_text, output_text) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def format_conversation(history):\n",
    "    # Concatenate the conversation history with prompts and responses\n",
    "    formatted_history = \"\"\n",
    "    for prompt, response in history:\n",
    "        formatted_history += alpaca_prompt.format(prompt, response)\n",
    "    return formatted_history\n",
    "\n",
    "def chat(model, tokenizer, max_history=5):\n",
    "    def send_message():\n",
    "        user_input = input_box.get()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            root.quit()\n",
    "\n",
    "        # Append the user input to the conversation history\n",
    "        conversation_history.append((user_input, \"\"))\n",
    "        \n",
    "        # Keep only the last `max_history` turns\n",
    "        conversation_history[:] = conversation_history[-max_history:]\n",
    "\n",
    "        # Format the conversation history for the model\n",
    "        formatted_history = format_conversation(conversation_history)\n",
    "        \n",
    "        # Tokenize the formatted history\n",
    "        inputs = tokenizer([formatted_history], return_tensors=\"pt\").to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Generate a response\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        # Decode the generated response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the bot's response after the last prompt\n",
    "        bot_response = response.split(alpaca_prompt.format(user_input, \"\"))[-1].strip()\n",
    "        \n",
    "        # Update the last user input with the model response in the conversation history\n",
    "        conversation_history[-1] = (user_input, bot_response)\n",
    "        \n",
    "        # Display the conversation\n",
    "        chat_history.config(state=tk.NORMAL)\n",
    "        chat_history.insert(tk.END, f\"You: {user_input}\\nBot: {bot_response}\\n\")\n",
    "        chat_history.config(state=tk.DISABLED)\n",
    "        chat_history.yview(tk.END)\n",
    "        \n",
    "        # Clear the input box\n",
    "        input_box.delete(0, tk.END)\n",
    "\n",
    "    # Setup tkinter GUI\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Chat with LLM\")\n",
    "\n",
    "    chat_history = scrolledtext.ScrolledText(root, wrap=tk.WORD, state=tk.DISABLED)\n",
    "    chat_history.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    input_box = tk.Entry(root, width=100)\n",
    "    input_box.pack(padx=10, pady=10, fill=tk.X, expand=True)\n",
    "    input_box.bind(\"<Return>\", lambda event: send_message())\n",
    "\n",
    "    send_button = tk.Button(root, text=\"Send\", command=send_message)\n",
    "    send_button.pack(padx=10, pady=10)\n",
    "\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
